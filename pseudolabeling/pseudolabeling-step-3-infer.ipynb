{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85245735",
   "metadata": {
    "papermill": {
     "duration": 0.004468,
     "end_time": "2024-04-07T03:56:54.751159",
     "exception": false,
     "start_time": "2024-04-07T03:56:54.746691",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117ad827",
   "metadata": {
    "papermill": {
     "duration": 0.003674,
     "end_time": "2024-04-07T03:56:54.758931",
     "exception": false,
     "start_time": "2024-04-07T03:56:54.755257",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Continuation of the pseudolabeling pipeline described in https://www.kaggle.com/code/reasat/pseudolabeling-step-1-download-speech-audio\n",
    "\n",
    "Model weight and inference notebook copied from: https://www.kaggle.com/competitions/bengaliai-speech/discussion/447970\n",
    "\n",
    "## STT Model:\n",
    "\n",
    "* OpenAI whisper-medium\n",
    "* Huggingface trainer\n",
    "* Trained on 8x 48GB RTX A6000\n",
    "* bs=8 and lr=1e-5\n",
    "* Train steps 50k\n",
    "* Spectrogram dithering\n",
    "* Spectrogram time and frequency masking\n",
    "* Resampling 16khz->8khz->16khz as augmentation\n",
    "* Inference with max_length=260, num_beams=4 and chunk_length_s=20.1s\n",
    "* Libsonic based speed/pitch augmentation\n",
    "* Datasets: OpenSLR 37, OpenSLR 53, MadASR, Shrutilipi, Macro, Kathbath, GoogleTTS generated audios and pseudo labeled YouTube videos\n",
    "\n",
    "## Punctuation Model:\n",
    "\n",
    "* AutoModelForTokenClassification google/muril-base-cased\n",
    "* Huggingface trainer\n",
    "* Labels: period, comma and question mark\n",
    "* bs=64, lr=2e-4 and max_seq_length=512\n",
    "* Ensemble of 4 models (using 6, 8, 11 and 12 layers of google/muril-base-cased)\n",
    "* Normalized IndicCorp v2 Bangla dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1dd2c790",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-07T03:56:54.767891Z",
     "iopub.status.busy": "2024-04-07T03:56:54.767563Z",
     "iopub.status.idle": "2024-04-07T03:57:27.875943Z",
     "shell.execute_reply": "2024-04-07T03:57:27.874751Z"
    },
    "papermill": {
     "duration": 33.115609,
     "end_time": "2024-04-07T03:57:27.878345",
     "exception": false,
     "start_time": "2024-04-07T03:56:54.762736",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cp: cannot stat '/kaggle/input/bengali-eval-data/predict.py': No such file or directory\r\n",
      "jiwer/\r\n",
      "jiwer/jiwer-2.3.0-py3-none-any.whl\r\n",
      "jiwer/python-Levenshtein-0.12.2.tar.gz\r\n",
      "jiwer/setuptools-65.3.0-py3-none-any.whl\r\n",
      "Looking in links: ./\r\n",
      "Processing ./jiwer/python-Levenshtein-0.12.2.tar.gz\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from python-Levenshtein==0.12.2) (69.0.3)\r\n",
      "Building wheels for collected packages: python-Levenshtein\r\n",
      "  Building wheel for python-Levenshtein (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \bdone\r\n",
      "\u001b[?25h  Created wheel for python-Levenshtein: filename=python_Levenshtein-0.12.2-cp310-cp310-linux_x86_64.whl size=79809 sha256=674f26f4a2b389ec89da380c7fbd375a2c1de249be6505abc4547e45e368f5fd\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/f1/ab/b1/90d2068d73d15e52c1a65676d269a9f043b61221a29f7298e7\r\n",
      "Successfully built python-Levenshtein\r\n",
      "Installing collected packages: python-Levenshtein\r\n",
      "  Attempting uninstall: python-Levenshtein\r\n",
      "    Found existing installation: python-Levenshtein 0.25.0\r\n",
      "    Uninstalling python-Levenshtein-0.25.0:\r\n",
      "      Successfully uninstalled python-Levenshtein-0.25.0\r\n",
      "Successfully installed python-Levenshtein-0.12.2\r\n",
      "Looking in links: ./\r\n",
      "Processing ./jiwer/jiwer-2.3.0-py3-none-any.whl\r\n",
      "Requirement already satisfied: python-Levenshtein==0.12.2 in /opt/conda/lib/python3.10/site-packages (from jiwer==2.3.0) (0.12.2)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from python-Levenshtein==0.12.2->jiwer==2.3.0) (69.0.3)\r\n",
      "Installing collected packages: jiwer\r\n",
      "Successfully installed jiwer-2.3.0\r\n"
     ]
    }
   ],
   "source": [
    "!cp /kaggle/input/bengali-eval-data/predict.py .\n",
    "\n",
    "!cp -r ../input/python-packages2 ./\n",
    "!tar xvfz ./python-packages2/jiwer.tgz\n",
    "!pip install ./jiwer/python-Levenshtein-0.12.2.tar.gz -f ./ --no-index\n",
    "!pip install ./jiwer/jiwer-2.3.0-py3-none-any.whl -f ./ --no-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1849ccb4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-07T03:57:27.890170Z",
     "iopub.status.busy": "2024-04-07T03:57:27.889859Z",
     "iopub.status.idle": "2024-04-07T03:58:16.770223Z",
     "shell.execute_reply": "2024-04-07T03:58:16.769295Z"
    },
    "papermill": {
     "duration": 48.888886,
     "end_time": "2024-04-07T03:58:16.772568",
     "exception": false,
     "start_time": "2024-04-07T03:57:27.883682",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.38.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-07 03:57:34.692018: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-07 03:57:34.692164: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-07 03:57:34.795374: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import glob\n",
    "\n",
    "MODEL = '/kaggle/input/bengali-ai-asr-submission/bengali-whisper-medium/'\n",
    "PUNCT_MODELS = [\n",
    "    '/kaggle/input/bengali-ai-asr-submission/punct-model-6layers/',\n",
    "    '/kaggle/input/bengali-ai-asr-submission/punct-model-8layers/',\n",
    "    '/kaggle/input/bengali-ai-asr-submission/punct-model-11layers/',\n",
    "    '/kaggle/input/bengali-ai-asr-submission/punct-model-12layers/'\n",
    "]\n",
    "CHUNK_LENGTH_S = 20.1\n",
    "ENABLE_BEAM = True\n",
    "\n",
    "\n",
    "PUNCT_WEIGHTS = [[1.0, 1.4, 1.0, 0.8]]\n",
    "\n",
    "if ENABLE_BEAM:\n",
    "    BATCH_SIZE = 4\n",
    "else:\n",
    "    BATCH_SIZE = 8\n",
    "\n",
    "\n",
    "DATASET_PATH = '/kaggle/input/yt-speech-chunks/chunks'\n",
    "    \n",
    "import csv\n",
    "import glob\n",
    "import shutil\n",
    "import librosa\n",
    "import argparse\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import transformers\n",
    "print(transformers.__version__)\n",
    "from transformers import pipeline, AutoModelForTokenClassification, AutoTokenizer\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "files = list(glob.glob(DATASET_PATH + '/' + '*.wav'))\n",
    "files += list(glob.glob(DATASET_PATH + '/' + '*.mp3'))\n",
    "\n",
    "# NOTE: running on a few samples for demonstration\n",
    "files = files[:10]\n",
    "\n",
    "files.sort()\n",
    "\n",
    "pipe = pipeline(task=\"automatic-speech-recognition\",\n",
    "                model=MODEL,\n",
    "                tokenizer=MODEL,\n",
    "                chunk_length_s=CHUNK_LENGTH_S, device=0, batch_size=BATCH_SIZE)\n",
    "pipe.model.config.forced_decoder_ids = pipe.tokenizer.get_decoder_prompt_ids(language=\"bn\", task=\"transcribe\")\n",
    "\n",
    "print(\"model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8c23d8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-07T03:58:16.785637Z",
     "iopub.status.busy": "2024-04-07T03:58:16.785049Z",
     "iopub.status.idle": "2024-04-07T03:58:16.791596Z",
     "shell.execute_reply": "2024-04-07T03:58:16.790713Z"
    },
    "papermill": {
     "duration": 0.014909,
     "end_time": "2024-04-07T03:58:16.793568",
     "exception": false,
     "start_time": "2024-04-07T03:58:16.778659",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fix_repetition(text, max_count):\n",
    "    uniq_word_counter = {}\n",
    "    words = text.split()\n",
    "    for word in text.split():\n",
    "        if word not in uniq_word_counter:\n",
    "            uniq_word_counter[word] = 1\n",
    "        else:\n",
    "            uniq_word_counter[word] += 1\n",
    "\n",
    "    for word, count in uniq_word_counter.items():\n",
    "        if count > max_count:\n",
    "            words = [w for w in words if w != word]\n",
    "    text = \" \".join(words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e61fb1d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-07T03:58:16.805320Z",
     "iopub.status.busy": "2024-04-07T03:58:16.805073Z",
     "iopub.status.idle": "2024-04-07T03:58:40.128741Z",
     "shell.execute_reply": "2024-04-07T03:58:40.127939Z"
    },
    "papermill": {
     "duration": 23.332022,
     "end_time": "2024-04-07T03:58:40.130984",
     "exception": false,
     "start_time": "2024-04-07T03:58:16.798962",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if ENABLE_BEAM:\n",
    "    texts = pipe(files, generate_kwargs={\"max_length\": 260, \"num_beams\": 4})\n",
    "else:\n",
    "    texts = pipe(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "052e0340",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-07T03:58:40.143498Z",
     "iopub.status.busy": "2024-04-07T03:58:40.143184Z",
     "iopub.status.idle": "2024-04-07T03:59:16.450847Z",
     "shell.execute_reply": "2024-04-07T03:59:16.450048Z"
    },
    "papermill": {
     "duration": 36.316616,
     "end_time": "2024-04-07T03:59:16.453251",
     "exception": false,
     "start_time": "2024-04-07T03:58:40.136635",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "del pipe\n",
    "import torch\n",
    "models = [\n",
    "    AutoModelForTokenClassification.from_pretrained(f).eval().cuda() for f in PUNCT_MODELS\n",
    "]\n",
    "tokenizer = AutoTokenizer.from_pretrained(PUNCT_MODELS[0])\n",
    "def punctuate(text):\n",
    "    input_ids = tokenizer(text).input_ids\n",
    "    with torch.no_grad():\n",
    "        model = models[0]\n",
    "        logits = torch.nn.functional.softmax(\n",
    "            model(input_ids=torch.LongTensor([input_ids]).cuda()).logits[0, 1:-1],\n",
    "            dim=1).cpu()\n",
    "        for model in models[1:]:\n",
    "            logits += torch.nn.functional.softmax(\n",
    "                model(input_ids=torch.LongTensor([input_ids]).cuda()).logits[0, 1:-1],\n",
    "                dim=1).cpu()\n",
    "        logits = logits / len(models)\n",
    "        logits *= torch.FloatTensor(PUNCT_WEIGHTS)\n",
    "        label_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        tokens = tokenizer(text, add_special_tokens=False).input_ids\n",
    "        punct_text = \"\"\n",
    "        for index, token in enumerate(tokens):\n",
    "            token_str = tokenizer.decode(token)\n",
    "            if '##' not in token_str:\n",
    "                punct_text += \" \" + token_str\n",
    "            else:\n",
    "                punct_text += token_str[2:]\n",
    "            punct_text += ['', '।', ',', '?'][label_ids[index].item()]\n",
    "\n",
    "    punct_text = punct_text.strip()\n",
    "    return punct_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fdd5038",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-07T03:59:16.465694Z",
     "iopub.status.busy": "2024-04-07T03:59:16.465356Z",
     "iopub.status.idle": "2024-04-07T03:59:16.810820Z",
     "shell.execute_reply": "2024-04-07T03:59:16.809832Z"
    },
    "papermill": {
     "duration": 0.353961,
     "end_time": "2024-04-07T03:59:16.813017",
     "exception": false,
     "start_time": "2024-04-07T03:59:16.459056",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference finished!\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "with open(\"submission.csv\", 'wt', encoding=\"utf8\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['id', 'sentence'])\n",
    "    for f, text in zip(files, texts):\n",
    "        file_id = Path(f).stem\n",
    "        pred = text['text'].strip()\n",
    "        pred = fix_repetition(pred, max_count=8)\n",
    "        pred = punctuate(pred)\n",
    "        if pred[-1] not in ['।', '?', ',']:\n",
    "            pred = pred + '।'\n",
    "        # print(i, file_id, pred)\n",
    "        prediction = [file_id, pred]\n",
    "        writer.writerow(prediction)\n",
    "        predictions.append(prediction)\n",
    "print(\"inference finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "740b15c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-07T03:59:16.825446Z",
     "iopub.status.busy": "2024-04-07T03:59:16.825152Z",
     "iopub.status.idle": "2024-04-07T03:59:16.833556Z",
     "shell.execute_reply": "2024-04-07T03:59:16.832701Z"
    },
    "papermill": {
     "duration": 0.017104,
     "end_time": "2024-04-07T03:59:16.835782",
     "exception": false,
     "start_time": "2024-04-07T03:59:16.818678",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"/kaggle/working/submission.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01525e48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-07T03:59:16.848063Z",
     "iopub.status.busy": "2024-04-07T03:59:16.847791Z",
     "iopub.status.idle": "2024-04-07T03:59:16.862086Z",
     "shell.execute_reply": "2024-04-07T03:59:16.861234Z"
    },
    "papermill": {
     "duration": 0.022779,
     "end_time": "2024-04-07T03:59:16.864135",
     "exception": false,
     "start_time": "2024-04-07T03:59:16.841356",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000000_000121</td>\n",
       "      <td>হামার ও তারুণ্যের কিতাবার্তা ভাইসাহাব আমি হলাম...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000000_000167</td>\n",
       "      <td>এক লাখ টাকায় বিপদের সাথে সাহায্য করছে।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000000_000205</td>\n",
       "      <td>কিন্তু আমার না হলে বড় তালায় ফাইনা দিব কোথায় আম...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000000_000206</td>\n",
       "      <td>যাইহোক আজকের লাগে ডিউটি শেষ আমার বেশ বুঝে একটু...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000000_000229</td>\n",
       "      <td>তাই আমার ব্যবসার লিয়ামাতে কত বড় তুই তো আমার সি...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>000000_000242</td>\n",
       "      <td>ওহ বাবা খুচরে যার জীবনীত সব ছাইবার করিয়া থেকে ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>000000_000285</td>\n",
       "      <td>ওই যাও তুমি। কুটিপতি দেওয়া বাবা ফকিডর গেছে। ফক...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>000001_000127</td>\n",
       "      <td>বাদ দিলেন না।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>000001_000203</td>\n",
       "      <td>আর তোমার এত ফলা ফৈসল যে পায় আমারে দোকা তুমি না...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>000001_000246</td>\n",
       "      <td>পিতারে আমার দেয়টা তার বাপ লাগে।</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                           sentence\n",
       "0  000000_000121  হামার ও তারুণ্যের কিতাবার্তা ভাইসাহাব আমি হলাম...\n",
       "1  000000_000167             এক লাখ টাকায় বিপদের সাথে সাহায্য করছে।\n",
       "2  000000_000205  কিন্তু আমার না হলে বড় তালায় ফাইনা দিব কোথায় আম...\n",
       "3  000000_000206  যাইহোক আজকের লাগে ডিউটি শেষ আমার বেশ বুঝে একটু...\n",
       "4  000000_000229  তাই আমার ব্যবসার লিয়ামাতে কত বড় তুই তো আমার সি...\n",
       "5  000000_000242  ওহ বাবা খুচরে যার জীবনীত সব ছাইবার করিয়া থেকে ...\n",
       "6  000000_000285  ওই যাও তুমি। কুটিপতি দেওয়া বাবা ফকিডর গেছে। ফক...\n",
       "7  000001_000127                                      বাদ দিলেন না।\n",
       "8  000001_000203  আর তোমার এত ফলা ফৈসল যে পায় আমারে দোকা তুমি না...\n",
       "9  000001_000246                    পিতারে আমার দেয়টা তার বাপ লাগে।"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89257b85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-07T03:59:16.877137Z",
     "iopub.status.busy": "2024-04-07T03:59:16.876891Z",
     "iopub.status.idle": "2024-04-07T03:59:16.881224Z",
     "shell.execute_reply": "2024-04-07T03:59:16.880376Z"
    },
    "papermill": {
     "duration": 0.013481,
     "end_time": "2024-04-07T03:59:16.883527",
     "exception": false,
     "start_time": "2024-04-07T03:59:16.870046",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b91505",
   "metadata": {
    "papermill": {
     "duration": 0.00556,
     "end_time": "2024-04-07T03:59:16.896484",
     "exception": false,
     "start_time": "2024-04-07T03:59:16.890924",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 2447262,
     "sourceId": 4143520,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3865741,
     "sourceId": 6707460,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4740850,
     "sourceId": 8041252,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30674,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 148.071275,
   "end_time": "2024-04-07T03:59:20.208413",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-07T03:56:52.137138",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
